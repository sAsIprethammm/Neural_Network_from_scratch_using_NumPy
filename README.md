# Neural_Network_from_scratch_using_NumPy
Developed a neural network from scratch using NumPy, implementing weight and bias initialization, forward and backward propagation, and various activation functions. Trained the network for classification and regression tasks, demonstrating foundational deep learning concepts without relying on high-level libraries like TensorFlow or PyTorch.

Key Features:

Initialization of Weights and Biases: Proper initialization techniques to ensure the network starts with suitable parameters.

Forward Propagation: Calculation of activations and output through each layer of the network.

Activation Functions: Implementation of commonly used activation functions like sigmoid, ReLU, and tanh.

Backward Propagation: Computation of gradients to update the weights and biases using backpropagation and gradient descent.

Loss Calculation: Implementation of loss functions such as Mean Squared Error (MSE) and Cross-Entropy Loss for evaluating the network's performance.

Training Loop: Iterative process to train the network over multiple epochs, adjusting the weights and biases to minimize the loss.

Evaluation: Methods to evaluate the networkâ€™s performance on training and test datasets.

Technologies Used:

Python
NumPy
Outcome:
The project provides a comprehensive understanding of the inner workings of neural networks and the foundational algorithms used in deep learning. By building a neural network from scratch, I gained valuable insights into how neural networks function and how to implement them without relying on advanced libraries.

This project serves as an excellent learning tool for anyone interested in understanding the basics of neural networks and deep learning. It also lays a strong foundation for exploring more complex deep learning models and frameworks in the future.
